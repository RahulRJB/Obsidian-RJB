
# RCNN


DATE:  04-09-24


Tags: [[Object Detection]]


# References:
- https://www.youtube.com/watch?v=nJzQDpppFj0
- 


# Content:

- [[CNN]] goal: Predict output in an image.
- Object detection models: Predict output in an image + bounding boxes.
- #### Idea of Bounding Boxes:
	- The CNN bit used ![[Attachments/Pasted image 20240904135328.png]]
	- For object detection task we have to modify it a little bit. After fully connected layer, we create 2 branches, 1st for CNN classification, and 2nd another fully connected layer to Output our bounding box coordinates. We already know what are the correct bounding boxes during training, use [[Notes/L2 loss (MSE loss)]]for the loss function(since its regression task). Lastly we use weighted sum of the losses to compute our final loss.![[Attachments/Pasted image 20240904135559.png]]
	- Disadvantage: Only can detect 1 object in the picture because we just output one single bounding box coordinates. Can't use this architecture in scenarios when multiple objects exist in our input. This architecture is just used for **object localization**, i.e. only 1 object in the image.

- #### Idea of Object Detection model:
	- ![[Attachments/Pasted image 20240911123128.png]]In an input image, we define a so-called sliding window. Start with putting it at the top-left corner of the image, then classify the region. So it extracts the region, passes it through a neural network classifier, i.e CNN modules and it produces C+1 output. **C**, num of classes we already know, and 1 for background. In above scenario, since it's the sky without anything so we expect our CNN modules to give us a background class.

	- ![[Attachments/Pasted image 20240911123234.png]]Then we slide our sliding window to every possible locations in the image, eg. here it extracts the region where there's a mountain in it.
	- ![[Attachments/Pasted image 20240911123413.png]] Finally we might reach a region with a car, extract it and classify it.
	- ![[Attachments/Pasted image 20240911123510.png]]**Problem with this model**: Imagine our bounding box has a width *w* and height *h* and image width is *W* and height is *H*. Then possible positions it has to try is `(W-w+1)*(H-h+1)`, which is a lot! Actually it is even worse, the bounding box width and height can change as we slide and that's just because we have to take different boundary boxes with different scales and different aspect ratios into accounts and when we do this, the total possibilities of bounding boxes is insane![[Attachments/Pasted image 20240911123620.png]]For example if the input image is 224x224,  then we have to examine around 635 million boxes which is not computationally feasible if you want to do this in real time.

- #### Idea of RCNN:
	- To tackle the above problem, instead of exploring every possible bounding boxes we can use an external algorithm to propose us some regions. The algorithm used is called [[Selective Search]] which has existed before! 
	- ![[Attachments/Pasted image 20240911124014.png]]Eg. Given above image, the Selective search algo might propose ~2000 bounding boxes that we call region proposals (ROI). The latency for this maybe 1-2seconds.
	- ![[Attachments/Pasted image 20240911124126.png]]So the pipeline is like first we take an image, we run **Selective Search** and it generates some region proposals (ROI). For the sake of simplicity let's assume we have only 2 ROIs. Next we extract these region proposals from the original image, transform them to be all squared and suitable for inputting to a CNN module and produces a class probability for the object in the region.
	- Is it enough, actually not because the bounding box generated by **Selective Search** algorithm may not be precise. For eg. the sleeves of the person above or ears of the horse is missing. So we add another branch that is responsible to tweak the bounding box a little bit and make it better!
	- ![[Attachments/Pasted image 20240911124754.png]]How this work? Our Selective Search algorithm gives us a region proposal described by px, py, ph and pw and our final output of our neural network generates a transform Vector tx, ty, th and tw such as to get the final bb, bx, by, bh and bw using above transformations, translation and log space scale transfer.
	- ![[Attachments/Pasted image 20240911125236.png]]If we output 2 boxes, both pointing to the same object, which one we should choose? Answer is **Non-max Suppression** for solving this issue. Since its supervised learning task, we already know what's the perfect bounding box is as we manually label it before training the model and we call it ground truth. Intuitively the one that is closer to the ground truth and has more overlapping is a better candidate, we quantify this using **IoU**.
	- ![[Attachments/Pasted image 20240911125358.png]]Intersection over Union(IoU). Since 0.48>0.15, the right one is having more overlap and we choose this and remove the left one.

- ![[Attachments/Pasted image 20240911125621.png]]If the bb are referring to different objects and we do not use non-max suppression even if the bbs are intersecting!
- ![[Attachments/Pasted image 20240911125921.png]] How to evaluate our model? The metric proposed in the context of object detection is called **Mean Average Precision(mAP)**. If we have the above dog picture, we already manually labeled it. Our model predicts 3 bbs. The question is which predicted bounding boxes are correct. For this, we take a look at their IOU for a given ground truth bb with all the predicted bbs, if larger than a threshold say 0.5, we'll label them as correct and otherwise we'll label them as wrong. Wrt the 1st dog in the picture, the 2nd one is correct and the rest are wrong. The metrics we use are precision and recall. 
- ![[Attachments/Pasted image 20240911130549.png]]Now let's understand how **mAP** works for multiple images. If we have 4 images and their corresponding ground truths of multiple classes, to calculate the mAP, we look at 1 class at a time. Let's choose dogs first, by giving these pictures to our model, it predicts some bbs with a probability of dog being existed in the area.
	- 1st step to calculate mAP is to sort these bbs based on their probability score and we also create a Precision vs Recall graph. 
	- We start with the leftmost bb having the most probability score. Does it have an IOU more than 0.5 with any of the ground truths? Probably YES, so we'll label it remove both the prediction and ground truth from the image since we don't care about them anymore. So Precision => 1/1=1, and Recall => 1/3=0.33. We draw our point on the graph.
	- For the 2nd bb, IoU < 0.5 with all ground truth. So it is incorrect and remove it from our image since we don't care about it anymore. Precision => 1/2=0.5, and Recall => 1/3=0.33. Again we draw our results on the graph.
	- For the 3rd bb let's assume it has IoU>0.5, we mark it correct. Precision => 2/3=0.66, and Recall => 2/3=0.66. Again we draw the results on the diagram.
	- Last one, it has IoU<0.5, we'll label it as Incorrect and we remove it from the image again. Precision => 2/4=0.5, and Recall => 2/3=0.66. We draw the final point.
	- Finally we compute the AUC and this gives us average Precision(AP)![[Attachments/Pasted image 20240911130828.png]]
- ![[Attachments/Pasted image 20240911131002.png]]All these calculations were for dog class for which AP=0.58. We repeat the same calculation for cat class, which may give us an AP=0.62. Then we take average of these 2 numbers and it gives us mAP@0.5.
- ![[Attachments/Pasted image 20240911131140.png]]This mAP is for threshold=0.5, you should try it with other IoU thresholds such as 0.55, 0.60, and continue doing it until a fixed value such as 0.95.Tthen we take average of all these numbers and we can say the MAP starting from 0.5 until 0.95 with the separation of 0.05 is 0.55. So **Average of mAP is MAP**.