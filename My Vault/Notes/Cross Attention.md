
# Cross Attention


DATE:  27-03-25


Tags:  [[Notes/Self Attention|Self Attention]] [[Notes/Transformers|Transformers]] [[Notes/LLMs|LLMs]]

# References:

https://www.youtube.com/watch?v=aw3H-wPuRcw


# Content:

Query is taken from the Encoder and Key and Value from Decoder. 
This is because The Encoder input is the full input and therefore it has to decide how and what to pay attention to in the decoder tokens to order to best generate the next token!



